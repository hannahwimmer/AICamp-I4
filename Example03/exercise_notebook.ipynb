{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3b0de3",
   "metadata": {},
   "source": [
    "# Notebook: Data Pipelines with Dagster\n",
    "\n",
    "## Section 1: Introduction\n",
    "\n",
    "In this notebook, we'll take a look at managing a real-world problem effectively. We'll\n",
    "focus on generating clean, reusable code and utilize Dagster for monitoring the whole\n",
    "data pipeline. \n",
    "\n",
    "### The example: Video sequencing, transcription, and summarization\n",
    "Sometimes, we don't have the time to listen to a video in full. As an example for a\n",
    "data pipelines and workflow management, we'll thus consider the following processes:\n",
    "\n",
    "- Video scene detection\n",
    "- Video segmentation\n",
    "- Transcription\n",
    "- Summarization\n",
    "\n",
    "In the following, we'll introduce the tools used in each step, and then show how to\n",
    "**orchestrate them** in a pipeline using Dagster.\n",
    "\n",
    "\n",
    "## Section 2: Tool Overview\n",
    "\n",
    "There are many off-the-shelf solutions for the problems mentioned. To solve our little\n",
    "problem of segmenting, transcribing, and summarizing a video, we'll use:\n",
    "\n",
    "1. **PySceneDetect** for scene detection in videos (https://www.scenedetect.com/)\n",
    "2. **MoviePy** for cutting and manipulating video files (https://pypi.org/project/moviepy/)\n",
    "3. **Faster Whisper** for audio transcription (https://docs.linuxserver.io/images/docker-faster-whisper/)\n",
    "4. **Ollama / LLMs** for text summarization (https://ollama.com/search)\n",
    "5. **Dagster** for pipeline orchestration (https://dagster.io/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f888207",
   "metadata": {},
   "source": [
    "## Scene Detection\n",
    "\n",
    "Scene Detection uses histogram changes across video frames to assess whether the setting\n",
    "(i.e., the *scene*) has changed considerably. You'll have seen this before on Youtube:\n",
    "\n",
    "https://www.youtube.com/watch?v=TZe5UqlUg0c\n",
    "\n",
    "Let's take a look on how that works in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eccde512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene 1: start 0.00s, end 7.60s\n",
      "Scene 2: start 7.60s, end 21.00s\n",
      "Scene 3: start 21.00s, end 6.60min\n",
      "Scene 4: start 6.60min, end 6.80min\n",
      "Scene 5: start 6.80min, end 13.28min\n",
      "Scene 6: start 13.28min, end 13.41min\n",
      "Scene 7: start 13.41min, end 18.23min\n"
     ]
    }
   ],
   "source": [
    "### Taking a closer look Scene Detection with PySceneDetect...\n",
    "from scenedetect import open_video, SceneManager\n",
    "from scenedetect.detectors import ContentDetector\n",
    "\n",
    "# select a sample video \n",
    "video_path = \"data/example_video.mp4\"\n",
    "video = open_video(video_path)\n",
    "\n",
    "# set up SceneDetect's Scene Manager\n",
    "scene_manager = SceneManager()\n",
    "scene_manager.add_detector(ContentDetector(threshold=27.0))\n",
    "\n",
    "# detect scenes in the video\n",
    "scene_manager.detect_scenes(video)\n",
    "scene_list = scene_manager.get_scene_list()\n",
    "scene_times = [(start.get_seconds(), end.get_seconds()) for start, end in scene_list]\n",
    "\n",
    "for k, scene_time in enumerate(scene_times):\n",
    "    start_time, end_time = scene_time\n",
    "    if start_time < 60 and end_time < 60:\n",
    "        print(f\"Scene {k+1}: start {scene_time[0]:.2f}s, end {scene_time[1]:.2f}s\")\n",
    "    elif start_time < 60 and end_time >= 60:\n",
    "        print(f\"Scene {k+1}: start {scene_time[0]:.2f}s, end {scene_time[1]/60:.2f}min\")\n",
    "    else:\n",
    "        print(f\"Scene {k+1}: start {scene_time[0]/60:.2f}min, end {scene_time[1]/60:.2f}min\")\n",
    "\n",
    "### output: temporal edges of each individual recognized scene!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795dd3d",
   "metadata": {},
   "source": [
    "## Video Segmentation\n",
    "\n",
    "Now that we know the edges of each scene, we can use this information to cut the\n",
    "original video into single scenes. For this, we'll use Moviepy.\n",
    "\n",
    "Let's see how that works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1cb23af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video data/scenes_demo/scene_1.mp4.\n",
      "MoviePy - Writing audio in scene_1TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video data/scenes_demo/scene_1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready data/scenes_demo/scene_1.mp4\n",
      "MoviePy - Building video data/scenes_demo/scene_2.mp4.\n",
      "MoviePy - Writing audio in scene_2TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video data/scenes_demo/scene_2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready data/scenes_demo/scene_2.mp4\n",
      "MoviePy - Building video data/scenes_demo/scene_3.mp4.\n",
      "MoviePy - Writing audio in scene_3TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video data/scenes_demo/scene_3.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready data/scenes_demo/scene_3.mp4\n",
      "MoviePy - Building video data/scenes_demo/scene_4.mp4.\n",
      "MoviePy - Writing audio in scene_4TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video data/scenes_demo/scene_4.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready data/scenes_demo/scene_4.mp4\n",
      "MoviePy - Building video data/scenes_demo/scene_5.mp4.\n",
      "MoviePy - Writing audio in scene_5TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video data/scenes_demo/scene_5.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready data/scenes_demo/scene_5.mp4\n",
      "MoviePy - Building video data/scenes_demo/scene_6.mp4.\n",
      "MoviePy - Writing audio in scene_6TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video data/scenes_demo/scene_6.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready data/scenes_demo/scene_6.mp4\n",
      "MoviePy - Building video data/scenes_demo/scene_7.mp4.\n",
      "MoviePy - Writing audio in scene_7TEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video data/scenes_demo/scene_7.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready data/scenes_demo/scene_7.mp4\n",
      "Scenes saved in data/scenes_demo\n"
     ]
    }
   ],
   "source": [
    "# testing video segmentation with Moviepy...\n",
    "from moviepy import VideoFileClip\n",
    "import os\n",
    "\n",
    "# let's first set up a folder to store the clipped videos in:\n",
    "output_dir = \"data/scenes_demo\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# import video as a VideoFileClip (base class for video clips in Moviepy)\n",
    "video_path = \"data/example_video.mp4\"\n",
    "video_clip = VideoFileClip(video_path)\n",
    "\n",
    "# for each single scene, we now \"clip\" (cut) the video_clip \n",
    "for idx, (start_time, end_time) in enumerate(scene_times):\n",
    "    subclip = video_clip.subclipped(start_time, end_time)\n",
    "    out_path = os.path.join(output_dir, f\"scene_{idx+1}.mp4\")\n",
    "    subclip.write_videofile(out_path)\n",
    "video_clip.close()\n",
    "\n",
    "print(f\"Scenes saved in {output_dir}\")\n",
    "\n",
    "### output: videos of the single scenes saved in out output folder!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5813a",
   "metadata": {},
   "source": [
    "## Video Transcription\n",
    "\n",
    "Now we segmented the videos into individual scenes and saved those as separate videos\n",
    "(for tracability). We don't want to listen to the scenes, though - we'd like to have\n",
    "the text transcribed for us.\n",
    "\n",
    "Here's how we'll go about this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2737a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-09-01 13:50:53.193] [ctranslate2] [thread 55501] [warning] The compute type inferred from the saved model is float16, but the target device or backend do not support efficient float16 computation. The model weights have been automatically converted to use the float32 compute type instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcripts saved in data/transcripts_demo\n"
     ]
    }
   ],
   "source": [
    "# video transcription using whisper...\n",
    "from faster_whisper import WhisperModel\n",
    "import os\n",
    "\n",
    "# we'll use a tiny Whisper model that can run locally without problems\n",
    "model = WhisperModel(\"tiny\", device=\"cpu\")\n",
    "\n",
    "# set up directories for input scenes and output transcripts\n",
    "scenes_dir = \"data/scenes_demo\"\n",
    "transcripts_dir = \"data/transcripts_demo\"\n",
    "os.makedirs(transcripts_dir, exist_ok=True)\n",
    "\n",
    "# for every video file in the scenes directory...\n",
    "for file_name in os.listdir(scenes_dir):\n",
    "    if file_name.lower().endswith(\".mp4\"):\n",
    "\n",
    "        #... we fetch input and set output file paths...\n",
    "        file_path = os.path.join(scenes_dir, file_name)\n",
    "        txt_filename = os.path.splitext(file_name)[0] + \".txt\"\n",
    "        txt_path = os.path.join(transcripts_dir, txt_filename)\n",
    "\n",
    "        #...we use the Whisper model to transcribe the video scene...\n",
    "        segments, _ = model.transcribe(file_path)\n",
    "        transcript = \"\\n\".join([segment.text.strip() for segment in segments])\n",
    "        \n",
    "        #...and we write the resulting text in a .txt file into the output folder\n",
    "        with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(transcript)\n",
    "\n",
    "print(f\"Transcripts saved in {transcripts_dir}\")\n",
    "\n",
    "### output: transcribed text of each single scene in separate .txt files!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c999c3",
   "metadata": {},
   "source": [
    "## Transcript summarization\n",
    "\n",
    "We finally got the scenes transcribed; now the only thing left is to summarize the\n",
    "transcripts to save us some reading time.\n",
    "\n",
    "Here's how we'll do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c97c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizing natural language texts with a small Ollama model (llama3.2:1b)\n",
    "from ollama import chat, ChatResponse\n",
    "\n",
    "# again, let's set up an output directory first...\n",
    "summary_dir = \"data/summaries_demo\"\n",
    "os.makedirs(summary_dir, exist_ok=True)\n",
    "\n",
    "# we'll need to give the LLM an instruction on what to do:\n",
    "prompt = f\"\"\"Summarize the following text. Do not verify facts, and do not add\n",
    "    commentary. Only output a concise summary in five sentences maximum. \"\"\"\n",
    "\n",
    "# look through all the .txt-files in transcripts_dir...\n",
    "for file_name in sorted(os.listdir(transcripts_dir)):\n",
    "    if file_name.lower().endswith(\".txt\"):\n",
    "\n",
    "        # fetch the specific transcript and extract the text\n",
    "        file_path = os.path.join(transcripts_dir, file_name)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            transcript = f.read()\n",
    "\n",
    "        # now we can call the LLM to summarize the transcript\n",
    "        response: ChatResponse = chat(\n",
    "            model='llama3.2:1b', \n",
    "            messages=[{'role': 'user', 'content': prompt + transcript}])\n",
    "        result = response[\"message\"][\"content\"]\n",
    "        txt_filename = os.path.splitext(file_name)[0] + \"_summary.txt\"\n",
    "        txt_path = os.path.join(summary_dir, txt_filename)\n",
    "        with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acd51eb",
   "metadata": {},
   "source": [
    "## General Statements\n",
    "\n",
    "We now build all of the parts we need for the entire process - however, we had to run it\n",
    "all manually in a sequence.\n",
    "\n",
    "We don't want that.\n",
    "\n",
    "To make everything reproducible and robust, we'll push all of the above into a clean\n",
    "pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cda9ec",
   "metadata": {},
   "source": [
    "## Let's talk about the pipeline, now\n",
    "\n",
    "Instead of running each step manually, we can define a **Dagster pipeline**:\n",
    "\n",
    "- Each step (scene detection, segmentation, transcription, summarization) is an **asset**.\n",
    "- Dagster handles dependencies and execution order.\n",
    "- Advantages:\n",
    "    - Reproducibility\n",
    "    - Observability\n",
    "    - Easy orchestration of complex workflows\n",
    "\n",
    "You can check the full Dagster pipeline in `pipeline.py`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "example03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
